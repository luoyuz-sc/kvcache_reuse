{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f3ca1a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HF_HOME\"] = \"/home/gehao/lyz/data/hf-cache\"\n",
    "os.environ[\"HF_ENDPOINT\"] = \"https://hf-mirror.com\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb5c7e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse, re, string, math, random\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "from collections import Counter\n",
    "import collections\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from typing import List, Tuple, Dict\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModelForQuestionAnswering,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorWithPadding,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    AutoModelForCausalLM\n",
    ")\n",
    "import evaluate\n",
    "import re\n",
    "from transformers.cache_utils import DynamicCache\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4cdeee35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess(generated: str) -> str:\n",
    "    # Take only the first non-empty line; trim common prefixes.\n",
    "    for line in generated.strip().splitlines():\n",
    "        ans = line.strip()\n",
    "        if not ans:\n",
    "            continue\n",
    "        ans = re.sub(r\"^(Answer|A|Assistant|Final Answer|</think>)\\s*[:\\-]\\s*\", \"\", ans, flags=re.I)\n",
    "        ans = ans.strip(\" #*`>\\\"'\")\n",
    "        return ans\n",
    "    return generated.strip()\n",
    "\n",
    "\n",
    "# ---------------- Generation ----------------\n",
    "@torch.no_grad()\n",
    "def sample_next_token(logits: torch.Tensor, temperature: float, top_p: float) -> int:\n",
    "    \"\"\"\n",
    "    logits: [vocab_size] (already the last time step)\n",
    "    returns: int token id\n",
    "    \"\"\"\n",
    "    if temperature <= 0.0:\n",
    "        idx = int(torch.argmax(logits, dim=-1).item())\n",
    "        return idx\n",
    "        \n",
    "    # Temperature\n",
    "    logits = logits / temperature\n",
    "\n",
    "    # Nucleus (top-p)\n",
    "    probs = torch.softmax(logits, dim=-1)\n",
    "    if 0.0 < top_p < 1.0:\n",
    "        sorted_probs, sorted_idx = torch.sort(probs, descending=True)\n",
    "        cumsum = torch.cumsum(sorted_probs, dim=-1)\n",
    "        mask = cumsum - sorted_probs > top_p\n",
    "        sorted_probs[mask] = 0\n",
    "        sorted_probs = sorted_probs / sorted_probs.sum()\n",
    "        choice = torch.multinomial(sorted_probs, num_samples=1)\n",
    "        return int(sorted_idx[choice].item())\n",
    "    else:\n",
    "        choice = torch.multinomial(probs, num_samples=1)\n",
    "        return int(choice.item())\n",
    "    \n",
    "@torch.no_grad()\n",
    "def sample_next_token_batch(logits: torch.Tensor, temperature: float, top_p: float) -> int:\n",
    "    \"\"\"\n",
    "    logits: [B, vocab_size] (already the last time step)\n",
    "    returns: [B] int token id\n",
    "    \"\"\"\n",
    "    if temperature <= 0.0:\n",
    "        return torch.argmax(logits, dim=-1).long().to(device)\n",
    "        \n",
    "    # Temperature\n",
    "    logits = logits / temperature\n",
    "\n",
    "    # Nucleus (top-p)\n",
    "    probs = torch.softmax(logits, dim=-1)\n",
    "    if 0.0 < top_p < 1.0:\n",
    "        sorted_probs, sorted_idx = torch.sort(probs, descending=True, dim=-1)\n",
    "        cumsum = torch.cumsum(sorted_probs, dim=-1)\n",
    "        mask = cumsum - sorted_probs > top_p\n",
    "        sorted_probs[mask] = 0\n",
    "        sorted_probs = sorted_probs / sorted_probs.sum(dim=-1, keepdim=True)\n",
    "        choice = torch.multinomial(sorted_probs, num_samples=1)\n",
    "        return choice.long().to(device)\n",
    "    else:\n",
    "        choice = torch.multinomial(probs, num_samples=1)\n",
    "        return choice.long().to(device)\n",
    "\n",
    "\n",
    "def reuse_layer(a_cache, b_cache, args):\n",
    "    \"\"\"\n",
    "    a_cache: [28, 2, [b, seqlen, 8, 128]]\n",
    "    b_cache: [36, 2, [b, seqlen, 8, 128]]\n",
    "\n",
    "    return: new_a_cache, reuse_b_layer_list\n",
    "    note that different layers in a_cache and b_cache may be on different devices\n",
    "    and new_a_cache should keep the same device placement as a_cache\n",
    "    \"\"\"\n",
    "    \n",
    "    def map_layer_nearest(idx_t, n_layers_s, n_layers_t):\n",
    "        if n_layers_t <= 1:\n",
    "            return 0\n",
    "        return int(round(idx_t * (n_layers_s - 1) / (n_layers_t - 1)))\n",
    "    \n",
    "    #print(f\"a_cache: {len(a_cache)} layers, b_cache: {len(b_cache)} layers\")\n",
    "    \n",
    "    reuse_a_layer_start = args.reuse_a_layer_start\n",
    "    a_kv_cache_list = [(a_cache[layer_idx][0].to(\"cuda:0\"), a_cache[layer_idx][1].to(\"cuda:0\")) for layer_idx in range(reuse_a_layer_start, len(a_cache))]\n",
    "    b_kv_cache_list = [(b_cache[layer_idx][0].to(\"cuda:0\"), b_cache[layer_idx][1].to(\"cuda:0\")) for layer_idx in range(len(b_cache))]\n",
    "\n",
    "    reuse_b_layer_list = [map_layer_nearest(layer_idx,len(a_cache),len(b_cache)) for layer_idx in range(reuse_a_layer_start, len(a_cache))]\n",
    "    reused_a_cache = [b_kv_cache_list[reuse_b_layer_list[i]] for i in range(len(reuse_b_layer_list))]\n",
    "    new_a_cache = a_cache[:reuse_a_layer_start] + tuple(reused_a_cache)\n",
    "    return new_a_cache, reuse_b_layer_list\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def kv_bridged_generate(model_t,model_s, tok, input_ids_list: list[int], args):\n",
    "    \"\"\"return: generated string\"\"\"\n",
    "\n",
    "    eos_id = tok.eos_token_id\n",
    "    max_new = args.max_new_tokens\n",
    "    temperature = args.temperature\n",
    "    top_p = args.top_p\n",
    "\n",
    "    input_ids = torch.tensor([input_ids_list], device=device)\n",
    "\n",
    "    # prefill with s\n",
    "    s_out = model_s(\n",
    "        input_ids=input_ids,\n",
    "        use_cache=True,\n",
    "        output_hidden_states=False,\n",
    "        output_attentions=False,\n",
    "    )\n",
    "    t_out = model_t(\n",
    "        input_ids=input_ids,\n",
    "        use_cache=True,\n",
    "        output_hidden_states=False,\n",
    "        output_attentions=False,\n",
    "    )\n",
    "    \n",
    "    pkv_s = tuple(tuple(t for t in layer) for layer in s_out.past_key_values)\n",
    "    pkv_t = tuple(tuple(t for t in layer) for layer in t_out.past_key_values)\n",
    "    \n",
    "    # substitue layer_a of a_cache with layer_b of b_cache\n",
    "    new_a_cache, reuse_b_layer_list = reuse_layer(pkv_s, pkv_t, args)\n",
    "    #print(f\"reuse_b_layer_list: {reuse_b_layer_list}\")\n",
    "    \n",
    "    first_token = sample_next_token(s_out.logits[:,-1,:].squeeze(0), temperature, top_p)\n",
    "    \n",
    "    past = DynamicCache.from_legacy_cache(past_key_values=new_a_cache)\n",
    "    generated = [first_token]\n",
    "    last_token = torch.tensor([[first_token]], device=device)\n",
    "    for _ in range(max_new-1):\n",
    "        out = model_t(\n",
    "            input_ids=last_token,\n",
    "            past_key_values=past,\n",
    "            use_cache=True,\n",
    "            output_attentions=False,\n",
    "            output_hidden_states=False,\n",
    "        )\n",
    "        logits = out.logits[:, -1, :].squeeze(0)  # [vocab]\n",
    "        past = out.past_key_values              # now past is from A going forward\n",
    "\n",
    "        next_id = sample_next_token(logits, temperature, top_p)\n",
    "        generated.append(next_id)\n",
    "\n",
    "        if next_id == eos_id:\n",
    "            break\n",
    "\n",
    "        # Prepare inputs for next step\n",
    "        last_token = torch.tensor([[next_id]], device=device)\n",
    "    \n",
    "    text = tok.decode([t for t in generated if t != eos_id], skip_special_tokens=True)\n",
    "    pos = text.index(\":\")\n",
    "    if pos>=0:\n",
    "        text=text[pos+1:].strip()\n",
    "    return postprocess(text)\n",
    "\n",
    "@torch.no_grad()\n",
    "def kv_bridged_generate_batch(model_t,model_s, tok, input_ids_list: list[list[int]], args):\n",
    "    \"\"\"return: generated string\"\"\"\n",
    "\n",
    "    eos_id = tok.eos_token_id\n",
    "    max_new = args.max_new_tokens\n",
    "    temperature = args.temperature\n",
    "    top_p = args.top_p\n",
    "\n",
    "    input_ids = torch.tensor(input_ids_list, device=device)\n",
    "\n",
    "    # prefill with s\n",
    "    s_out = model_s(\n",
    "        input_ids=input_ids,\n",
    "        use_cache=True,\n",
    "        output_hidden_states=False,\n",
    "        output_attentions=False,\n",
    "    )\n",
    "    t_out = model_t(\n",
    "        input_ids=input_ids,\n",
    "        use_cache=True,\n",
    "        output_hidden_states=False,\n",
    "        output_attentions=False,\n",
    "    )\n",
    "    \n",
    "    pkv_s = tuple(tuple(t for t in layer) for layer in s_out.past_key_values)\n",
    "    pkv_t = tuple(tuple(t for t in layer) for layer in t_out.past_key_values)\n",
    "    \n",
    "    # substitue layer_a of a_cache with layer_b of b_cache\n",
    "    new_a_cache, reuse_b_layer_list = reuse_layer(pkv_s, pkv_t, args)\n",
    "    #print(f\"reuse_b_layer_list: {reuse_b_layer_list}\")\n",
    "    \n",
    "    first_token = sample_next_token_batch(s_out.logits[:,-1,:], temperature, top_p)\n",
    "    \n",
    "    past = DynamicCache.from_legacy_cache(past_key_values=new_a_cache)\n",
    "    generated = [first_token.squeeze(-1).tolist()]  # [B]\n",
    "    last_token = first_token  # [B, 1]\n",
    "    print(last_token.shape)\n",
    "    for _ in range(max_new-1):\n",
    "        out = model_t(\n",
    "            input_ids=last_token,\n",
    "            past_key_values=past,\n",
    "            use_cache=True,\n",
    "            output_attentions=False,\n",
    "            output_hidden_states=False,\n",
    "        )\n",
    "        logits = out.logits[:, -1, :]  # [vocab]\n",
    "        past = out.past_key_values              # now past is from A going forward\n",
    "\n",
    "        next_id = sample_next_token_batch(logits, temperature, top_p)\n",
    "        generated.append(next_id.squeeze(-1).tolist())  # [B]\n",
    "\n",
    "        if all([i==eos_id for i in generated[-1]]):\n",
    "            break\n",
    "\n",
    "        # Prepare inputs for next step\n",
    "        last_token = next_id\n",
    "    \n",
    "    # generated: [seq_len, B] -> [B, seq_len]\n",
    "    generated = list(map(list, zip(*generated)))\n",
    "    print(\"generated:\",torch.tensor(generated).shape)\n",
    "    # decode each generated[i]\n",
    "    text = []\n",
    "    for i in range(len(generated)):\n",
    "        generated[i] = [t for t in generated[i] if t != eos_id]\n",
    "        text.append(tok.decode(generated[i], skip_special_tokens=True))\n",
    "    print(\"text:\",text)\n",
    "    return [postprocess(t) for t in text]\n",
    "\n",
    "\n",
    "def _white_space_fix(text: str) -> str: return \" \".join(text.split())\n",
    "def _remove_articles(text: str) -> str: return re.sub(r\"\\b(a|an|the)\\b\", \" \", text)\n",
    "def _remove_punc(text: str) -> str: return text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "\n",
    "def normalize(text: str) -> str:\n",
    "    return _white_space_fix(_remove_articles(_remove_punc(text.lower())))\n",
    "\n",
    "def exact_match(pred: str, gold: str) -> float:\n",
    "    return float(normalize(pred) == normalize(gold))\n",
    "\n",
    "def f1_score(pred: str, gold: str) -> float:\n",
    "    p_tokens, g_tokens = normalize(pred).split(), normalize(gold).split()\n",
    "    if len(p_tokens) == 0 or len(g_tokens) == 0:\n",
    "        return float(p_tokens == g_tokens)\n",
    "    common = sum((Counter(p_tokens) & Counter(g_tokens)).values())\n",
    "    if common == 0:\n",
    "        return 0.0\n",
    "    precision = common / len(p_tokens)\n",
    "    recall = common / len(g_tokens)\n",
    "    return 2 * precision * recall / (precision + recall)\n",
    "\n",
    "\n",
    "def format_context(example: Dict) -> str:\n",
    "    \"\"\"Compact, readable multi-hop context.\"\"\"\n",
    "    titles = example[\"context\"][\"title\"]\n",
    "    sents = example[\"context\"][\"sentences\"]\n",
    "    sections = []\n",
    "    for t, ss in list(zip(titles, sents)):\n",
    "        sections.append(f\"- {t}: {ss}\")\n",
    "    return \"\\n\".join(sections)\n",
    "\n",
    "INSTRUCT_HEADER = (\n",
    "    \"You are a precise question answering assistant. Use the CONTEXT to answer the QUESTION.\\n\"\n",
    "    \"Return the **shortest** possible answer begin with 'Answer: ' (e.g., single entity or 'yes'/'no'); no explanation.\\n\"\n",
    ")\n",
    "\n",
    "def get_input_ids_list(tokenizer, example: Dict) -> list[int]:\n",
    "    ctx = format_context(example)\n",
    "    q = example[\"question\"]\n",
    "    sys = INSTRUCT_HEADER.strip()\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": sys},\n",
    "        {\"role\": \"user\", \"content\": f\"CONTEXT:\\n{ctx}\\n\\nQUESTION: {q}\\n\"}\n",
    "    ]\n",
    "    prompt_str = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        enable_thinking=False # Switches between thinking and non-thinking modes. Default is True.\n",
    "    )\n",
    "    return tokenizer(prompt_str).input_ids\n",
    "\n",
    "def eval_one(example: Dict, tok, model_t, model_s, args) -> Tuple[str, float, float, str]:\n",
    "    input_ids_list = get_input_ids_list(tok, example)\n",
    "    pred = kv_bridged_generate(model_t,model_s, tok, input_ids_list, args)\n",
    "    gold = example[\"answer\"]\n",
    "    with open(f\"debug.log\", \"a\") as f:\n",
    "        f.write(f\"Q: {example['question']}\\nA: {pred}\\nG: {gold}\\n\")\n",
    "    return pred, exact_match(pred, gold), f1_score(pred, gold)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b14771",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(model_t, model_s, tokenizer):\n",
    "    \n",
    "    \n",
    "    args=argparse.Namespace(\n",
    "        dataset_config=\"distractor\",\n",
    "        max_new_tokens=200,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        device=None,\n",
    "        reuse_a_layer_start=0,  # for Qwen3-1.7B and Qwen3-0.6B\n",
    "    )\n",
    "    \n",
    "    data_files = {\n",
    "        \"validation\": \"/home/gehao/lyz/validation-00000-of-00001.parquet\"\n",
    "    }\n",
    "    ds = load_dataset(\"parquet\", data_files=data_files)\n",
    "    val = ds[\"validation\"]\n",
    "    print(\"val:\",val)\n",
    "\n",
    "    # load tokenizer and model\n",
    "    device = torch.device(args.device) if args.device else (torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\"))\n",
    "\n",
    "    predictions = {}\n",
    "    references = {}\n",
    "\n",
    "    # iterate examples (you can change to subset for quick tests)\n",
    "    EM,F1=0.0,0.0\n",
    "    cnt=0\n",
    "    for ex in tqdm(val):\n",
    "        if cnt>=100:  # for quick tests\n",
    "            break\n",
    "        pred, em ,f1 = eval_one(ex, tokenizer, model_t,model_s, args)\n",
    "        EM+=em\n",
    "        F1+=f1\n",
    "        cnt+=1\n",
    "        predictions[ex[\"id\"]] = pred\n",
    "        references[ex[\"id\"]] = ex[\"answer\"]\n",
    "\n",
    "    EM/=cnt\n",
    "    F1/=cnt\n",
    "    print(f\"EM: {EM*100:.2f}, F1: {F1*100:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "31cf641e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2b37ec747384e92a4090720cd853cc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Qwen3ForCausalLM(\n",
       "  (model): Qwen3Model(\n",
       "    (embed_tokens): Embedding(151936, 2048)\n",
       "    (layers): ModuleList(\n",
       "      (0-27): 28 x Qwen3DecoderLayer(\n",
       "        (self_attn): Qwen3Attention(\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (k_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "          (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "        )\n",
       "        (mlp): Qwen3MLP(\n",
       "          (gate_proj): Linear(in_features=2048, out_features=6144, bias=False)\n",
       "          (up_proj): Linear(in_features=2048, out_features=6144, bias=False)\n",
       "          (down_proj): Linear(in_features=6144, out_features=2048, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen3RMSNorm((2048,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen3RMSNorm((2048,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen3RMSNorm((2048,), eps=1e-06)\n",
       "    (rotary_emb): Qwen3RotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=151936, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name_ = \"Qwen/Qwen3-1.7B\"\n",
    "tokenizer_ = AutoTokenizer.from_pretrained(model_name_)\n",
    "model_ = AutoModelForCausalLM.from_pretrained(model_name_).to(device)\n",
    "model_.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "334fc6d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen3ForCausalLM(\n",
       "  (model): Qwen3Model(\n",
       "    (embed_tokens): Embedding(151936, 1024)\n",
       "    (layers): ModuleList(\n",
       "      (0-27): 28 x Qwen3DecoderLayer(\n",
       "        (self_attn): Qwen3Attention(\n",
       "          (q_proj): Linear(in_features=1024, out_features=2048, bias=False)\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
       "          (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "          (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "        )\n",
       "        (mlp): Qwen3MLP(\n",
       "          (gate_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (up_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (down_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
       "    (rotary_emb): Qwen3RotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=151936, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = \"Qwen/Qwen3-0.6B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d11751a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qwen3Config {\n",
      "  \"architectures\": [\n",
      "    \"Qwen3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"dtype\": \"float32\",\n",
      "  \"eos_token_id\": 151645,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_types\": [\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\"\n",
      "  ],\n",
      "  \"max_position_embeddings\": 40960,\n",
      "  \"max_window_layers\": 28,\n",
      "  \"model_type\": \"qwen3\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 28,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"transformers_version\": \"4.56.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 151936\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(model.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b303f402",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 6/7405 [00:03<1:13:29,  1.68it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmodel_\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[8], line 31\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(model_t, model_s, tokenizer)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cnt\u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m:  \u001b[38;5;66;03m# for quick tests\u001b[39;00m\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m pred, em ,f1 \u001b[38;5;241m=\u001b[39m \u001b[43meval_one\u001b[49m\u001b[43m(\u001b[49m\u001b[43mex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_t\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmodel_s\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m EM\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39mem\n\u001b[1;32m     33\u001b[0m F1\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39mf1\n",
      "Cell \u001b[0;32mIn[7], line 281\u001b[0m, in \u001b[0;36meval_one\u001b[0;34m(example, tok, model_t, model_s, args)\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21meval_one\u001b[39m(example: Dict, tok, model_t, model_s, args) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mstr\u001b[39m]:\n\u001b[1;32m    280\u001b[0m     input_ids_list \u001b[38;5;241m=\u001b[39m get_input_ids_list(tok, example)\n\u001b[0;32m--> 281\u001b[0m     pred \u001b[38;5;241m=\u001b[39m \u001b[43mkv_bridged_generate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_t\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmodel_s\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtok\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_ids_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    282\u001b[0m     gold \u001b[38;5;241m=\u001b[39m example[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124manswer\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    283\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdebug.log\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "File \u001b[0;32m/home/gehao/anaconda/anaconda3/envs/lyz/lib/python3.10/site-packages/torch/utils/_contextlib.py:120\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 120\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[7], line 127\u001b[0m, in \u001b[0;36mkv_bridged_generate\u001b[0;34m(model_t, model_s, tok, input_ids_list, args)\u001b[0m\n\u001b[1;32m    124\u001b[0m new_a_cache, reuse_b_layer_list \u001b[38;5;241m=\u001b[39m reuse_layer(pkv_s, pkv_t, args)\n\u001b[1;32m    125\u001b[0m \u001b[38;5;66;03m#print(f\"reuse_b_layer_list: {reuse_b_layer_list}\")\u001b[39;00m\n\u001b[0;32m--> 127\u001b[0m first_token \u001b[38;5;241m=\u001b[39m \u001b[43msample_next_token\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms_out\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlogits\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    129\u001b[0m past \u001b[38;5;241m=\u001b[39m DynamicCache\u001b[38;5;241m.\u001b[39mfrom_legacy_cache(past_key_values\u001b[38;5;241m=\u001b[39mnew_a_cache)\n\u001b[1;32m    130\u001b[0m generated \u001b[38;5;241m=\u001b[39m [first_token]\n",
      "File \u001b[0;32m/home/gehao/anaconda/anaconda3/envs/lyz/lib/python3.10/site-packages/torch/utils/_contextlib.py:120\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 120\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[7], line 36\u001b[0m, in \u001b[0;36msample_next_token\u001b[0;34m(logits, temperature, top_p)\u001b[0m\n\u001b[1;32m     34\u001b[0m     sorted_probs \u001b[38;5;241m=\u001b[39m sorted_probs \u001b[38;5;241m/\u001b[39m sorted_probs\u001b[38;5;241m.\u001b[39msum()\n\u001b[1;32m     35\u001b[0m     choice \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmultinomial(sorted_probs, num_samples\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 36\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mint\u001b[39m(\u001b[43msorted_idx\u001b[49m\u001b[43m[\u001b[49m\u001b[43mchoice\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     38\u001b[0m     choice \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmultinomial(probs, num_samples\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "main(model,model_,tokenizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (lyz)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
